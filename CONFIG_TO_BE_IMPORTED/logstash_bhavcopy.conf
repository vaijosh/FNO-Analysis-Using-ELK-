# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
	file {
		path => "E:/FNOAnalysisUsingELK/bhavcopy/*.csv"
		start_position => beginning
	}
}
filter {
    csv {
        columns => [
                "INSTRUMENT",
				"SYMBOL",
				"EXPIRY_DT",
				"STRIKE_PR",
				"OPTION_TYP",
				"OPEN",
				"HIGH",
				"LOW",
				"CLOSE",
				"SETTLE_PR",
				"CONTRACTS",
				"VAL_INLAKH",
				"OPEN_INT",
				"CHG_IN_OI",
				"TIMESTAMP"
        ]
		separator => ","
    }
	prune {
		whitelist_names  => [ 
			"INSTRUMENT",
			"SYMBOL",
			"EXPIRY_DT",
			"STRIKE_PR",
			"OPTION_TYP",
			"OPEN_INT",
			"CHG_IN_OI",
			"TIMESTAMP",
			"CLOSE",
			"SETTLE_PR"
		]
	}
	prune {
		blacklist_values => [ "INSTRUMENT", "INSTRUMENT",
						  "SYMBOL", "SYMBOL",
						  "EXPIRY_DT", "EXPIRY_DT",
						  "STRIKE_PR","STRIKE_PR",
						  "OPTION_TYP", "OPTION_TYP",
						  "OPEN", "OPEN",
						  "HIGH", "HIGH",
						  "LOW", "LOW",
						  "CLOSE", "CLOSE",
						  "SETTLE_PR", "SETTLE_PR",
						  "CONTRACTS", "CONTRACTS",
						  "VAL_INLAKH", "VAL_INLAKH",
						  "OPEN_INT", "OPEN_INT",
						  "CHG_IN_OI", "CHG_IN_OI",
						  "TIMESTAMP", "TIMESTAMP"
						  ]
	}
	
	mutate {
		uppercase => ["EXPIRY_DT"]
	}
	date {
		match=> ["EXPIRY_DT","dd-MMM-yyyy"]
		target=> "EXPIRY_DT"
	}
	date {
		match=> ["TIMESTAMP","dd-MMM-yyyy"]
		target=> "TIMESTAMP"
	}
}
output {
  elasticsearch {
	hosts => ["http://localhost:9200"]
    index => "bhavcopy"
  }
}